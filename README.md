# ğŸ§  LLM Arena â€” AI Model Comparator

LLM Arena is a web-based Django app that allows users to compare the responses of top AI models like GPT-4o, Gemini 2.5, Mistral 12B, and LLaMA 3 to the same user query â€” and then intelligently score and evaluate them for clarity, accuracy, relevance, and depth.

---

## ğŸš€ Features

- ğŸ” User login/signup system with per-user dashboard
- ğŸ’¬ Compare responses from multiple LLMs
- ğŸ“Š Gemini evaluates and scores all model responses
- ğŸ—‚ï¸ Users can view all their past queries and responses
- ğŸ“ˆ Admin-only analytics dashboard
  - Query trends
  - Model popularity
  - User-wise usage
- âœ¨ 2x2 layout with copy buttons and "Show More" toggles
- â³ Query limit per user (10 per 12 hrs)
- ğŸ“‚ Responses stored with timestamps



## âš™ï¸ Tech Stack

- ğŸ Python 3
- ğŸŒ Django 5.x
- ğŸ§  OpenAI, Gemini, Mistral, Groq (LLaMA)
- ğŸ—ƒï¸ SQLite (can be switched to PostgreSQL easily)
- ğŸ“Š Chart.js for visual analytics
- ğŸ¨ Custom CSS styling (no Bootstrap)

---

## ğŸ› ï¸ How to Run Locally

1. Clone the repo:

2. Create a virtual environment:

python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

3. Install dependencies:

pip install -r requirements.txt

4. Add your API keys to a .env file:

OPENAI_API_KEY=...
GOOGLE_API_KEY=...
MISTRAL_API_KEY=...
GROQ_API_KEY=...
CO_API_KEY=...


5. Run migrations:

python manage.py migrate


6. Run the app:

python manage.py runserver


## âœ¨ Planned Features:
1.ğŸ’¾ Export query results to PDF
2.ğŸ§  Model-wise comparison graphs
3.ğŸŒ Deployment on Render/EC2
4.ğŸŒ“ Dark mode toggle
5.ğŸ”„ Editable query re-run


## ğŸ¤ Contributing
Pull requests are welcome! For major changes, open an issue first to discuss what you'd like to change or add.

## ğŸ“œ License
MIT License â€” use it freely, modify it, give credit when you can :)

## ğŸ™Œ Author
Made with â¤ï¸ by Aaryan Kale







